{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVpWkYlOeryZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_to_file = '/content/lyrics_dataset.txt'"
      ],
      "metadata": {
        "id": "vBvVw-AMfIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Opening the text file in read mode and standard encoding it\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsiI4AsBfOf7",
        "outputId": "d730fcc4-58aa-49bc-d870-100d47ec1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 404640 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A look at the first 250 characters in text\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy0Z3GxpfU-u",
        "outputId": "2473b850-cf77-4a04-a28b-556bab20a574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I hate you for what you did\r\n",
            "And I miss you like a little kid\r\n",
            "I faked it every time\r\n",
            "But that's alr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GptgmEgffYp8",
        "outputId": "47d19a6e-4574-4335-de7e-98f9271f9d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "\n",
        "print('{ ===========>')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n==========>}')\n",
        "\n",
        "\n",
        "# Show how the first 20 characters from the text are mapped to integers\n",
        "print ('{} ==> characters mapped to int ==> {}'.format(repr(text[:20]), text_as_int[:20]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16BrwQ1XfdxE",
        "outputId": "40cd8f1a-f93f-4717-e0ee-0f0b18e7c2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ ===========>\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '&' :   5,\n",
            "  \"'\" :   6,\n",
            "  '(' :   7,\n",
            "  ')' :   8,\n",
            "  '*' :   9,\n",
            "  ',' :  10,\n",
            "  '-' :  11,\n",
            "  '.' :  12,\n",
            "  '/' :  13,\n",
            "  '0' :  14,\n",
            "  '1' :  15,\n",
            "  '2' :  16,\n",
            "  '3' :  17,\n",
            "  '4' :  18,\n",
            "  '5' :  19,\n",
            "  ...\n",
            "==========>}\n",
            "'I hate you for what ' ==> characters mapped to int ==> [35  2 61 54 73 58  2 78 68 74  2 59 68 71  2 76 61 54 73  2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()] , end = \"\")\n",
        "\n",
        "\n",
        "# Using batch method converted individual characters to sequences of desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyssnH09fk1o",
        "outputId": "77c45620-e319-4000-9798-39cde740b11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I hat\"I hate you for what you did\\r\\nAnd I miss you like a little kid\\r\\nI faked it every time\\r\\nBut that's alri\"\n",
            "'ght\\r\\nI can hardly feel anything\\r\\nI hardly feel anything at all\\r\\nYou gave me fifteen hundred\\r\\nTo see y'\n",
            "'our hypnotherapist\\r\\nI only went one time\\r\\nYou let it slide\\r\\nFell on hard times a year ago\\r\\nWas hoping'\n",
            "' you would let it go, and you did\\r\\nI have emotional motion sickness\\r\\nSomebody roll the windows down\\r\\n'\n",
            "\"There are no words in the English language\\r\\nI could scream to drown you out\\r\\nI'm on the outside looki\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZKRxZ6YfpBq",
        "outputId": "a2147266-eef4-461f-b3e1-84acab6ae51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  \"I hate you for what you did\\r\\nAnd I miss you like a little kid\\r\\nI faked it every time\\r\\nBut that's alr\"\n",
            "Target data: \" hate you for what you did\\r\\nAnd I miss you like a little kid\\r\\nI faked it every time\\r\\nBut that's alri\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1500 # keep between (1024 -> 1800) for best results\n"
      ],
      "metadata": {
        "id": "MwlxpgQ1fs1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "    tf.keras.layers.Dense(vocab_size,activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRJJAJitfxXK",
        "outputId": "508f1ff8-b798-4d48-ff37-537c455c7b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 98) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "\n",
        "sampled_indices\n",
        "\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvncZoMHf83L",
        "outputId": "b01e71df-5e19-496b-cffa-775953b5f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           25088     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1500)          7911000   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 98)            147098    \n",
            "                                                                 \n",
            " dropout (Dropout)           (64, None, 98)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8083186 (30.83 MB)\n",
            "Trainable params: 8083186 (30.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Input: \n",
            " \"mes our way\\r\\nAnd we ride down the kings highway\\r\\n\\r\\nNo you can't hide out\\r\\nIn a six gun town\\r\\nWe wann\"\n",
            "\n",
            "Next Char Predictions: \n",
            " 'wD\\n¡bksnq{Jp2”y0c–1Uo\\u205f&!b&B)Xd(l\\nFQN3SY&)hD&XN“ws–ab2IGMshmL’rí87 (á!\\u2005DNfT54NJ0xR]—\"Jql“VtT5vE/—0…\\r1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3KT9HGGf_xU",
        "outputId": "49e42580-23f5-4add-f727-dfee2b291796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 98)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.5846987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "EWFcKlKOgE--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "kcNbshLrgLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "EPOCHS=5\n",
        "\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EzIqXsogPDv",
        "outputId": "5abef0f2-b9a5-4b25-e951-d00a08169205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "62/62 [==============================] - 612s 10s/step - loss: 3.7131\n",
            "Epoch 2/5\n",
            "62/62 [==============================] - 580s 9s/step - loss: 2.9082\n",
            "Epoch 3/5\n",
            "62/62 [==============================] - 571s 9s/step - loss: 2.7391\n",
            "Epoch 4/5\n",
            "62/62 [==============================] - 568s 9s/step - loss: 2.6070\n",
            "Epoch 5/5\n",
            "62/62 [==============================] - 569s 9s/step - loss: 2.5087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, chars_to_generate , temp , start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = chars_to_generate\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "nOuPvW_bgUa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7iHHDSvozR91",
        "outputId": "250ae71b-9dce-4902-8ac1-e33e8601b7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "vVaa9ITGzWmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, chars_to_generate , temp , start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = chars_to_generate\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ],
      "metadata": {
        "id": "6JtPjWzkzeSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import arange\n",
        "\n",
        "# Number of characters to generate (keep between 250 to 500)\n",
        "chars_to_generate = 500\n",
        "\n",
        "# Printing the generated text\n",
        "# Temperature 1.0 gives the craziest output and 0.1 gives the lowest varience\n",
        "# Keeping the temperature 0.35 gives best meaningful / coherent text.\n",
        "\n",
        "# Give the seed string as the first word of generate text\n",
        "print(generate_text(model , chars_to_generate , 0.35 , start_string=u\"brook\"))\n",
        "\n",
        "# Uncomment below to check the variences ==>\n",
        "\n",
        "# for i in arange(0.1,1.1,0.1):\n",
        "#   print(\"==============\")\n",
        "#   print(\"FOR TEMP : {} \".format(i))\n",
        "#   print(\"==============\")\n",
        "#   print(generate_text(model , chars_to_generate , i , start_string=u\"Love \"))\n",
        "#   print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBKUuNu_zhF9",
        "outputId": "22d3438e-18b5-4001-a364-6146e4a5204d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slipper fill in the way with me poon\r\n",
            "The love, we'll be my beade\r\n",
            "I don't need you were me pace to the way the love\r\n",
            "It's not the light with my light\r\n",
            "\r\n",
            "\r\n",
            "I could be with the love wern the plawe\r\n",
            "\r\n",
            "\r\n",
            "So I don't need you were me heart\r\n",
            "\r\n",
            "\r\n",
            "I don't wenna the cander come come in my headted\r\n",
            "\r\n",
            "\r\n",
            "It's not the paster, when I'm sood me love\r\n",
            "\r\n",
            "\r\n",
            "It's not I want a fall for a pristed the wind of my heart and she love\r\n",
            "I can't heart in my the wante to the roon\r\n",
            "\r\n",
            "\r\n",
            "There when I can't not the cand to the plowe w\n"
          ]
        }
      ]
    }
  ]
}